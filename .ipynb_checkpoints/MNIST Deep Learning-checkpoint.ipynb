{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "\n",
    "#The Row dimention is \"None\". That is number data to train MNISt in one batch\n",
    "#Since we are not sure of the number of data, by putting 'None', tensorflow is going to figure it out for us\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Layers of hidden layers\n",
    "\n",
    "#layer 1\n",
    "W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "L1 = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "#layer 2\n",
    "W2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2))\n",
    "L2 = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "#output\n",
    "W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
    "model = tf.matmul(L2, W3)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Avg. cost = 0.428\n",
      "Epoch: 0002 Avg. cost = 0.167\n",
      "Epoch: 0003 Avg. cost = 0.117\n",
      "Epoch: 0004 Avg. cost = 0.088\n",
      "Epoch: 0005 Avg. cost = 0.071\n",
      "Epoch: 0006 Avg. cost = 0.061\n",
      "Epoch: 0007 Avg. cost = 0.054\n",
      "Epoch: 0008 Avg. cost = 0.046\n",
      "Epoch: 0009 Avg. cost = 0.044\n",
      "Epoch: 0010 Avg. cost = 0.037\n",
      "Epoch: 0011 Avg. cost = 0.034\n",
      "Epoch: 0012 Avg. cost = 0.032\n",
      "Epoch: 0013 Avg. cost = 0.029\n",
      "Epoch: 0014 Avg. cost = 0.028\n",
      "Epoch: 0015 Avg. cost = 0.023\n",
      "optimization complete\n",
      "test accuracy: 0.9817\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-3b0dda0e7971>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0msubplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0msubplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0msubplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAE8AAAB4CAYAAACzS6+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAVVJREFUeJzt3bFNxUAQQEEfooRPzPVfi10EMfRwxGT2f0JgaSbeYPWkjXestTae8/LXC9yZeIF4gXiBeIF4gXiBeIF4weuV4cfjseacv7TK/3Acx9da6+3M7KV4c85t3/fntrqJMcbH2VlnG4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4gXiBeIF4y11vnhMT63bTv9efim3s++pb4Uj5+cbSBeIF4gXiBeIF4gXiBeIF7wDTX9GpHm3LviAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size = 100\n",
    "    total_batch = int(mnist.train.num_examples /  batch_size)\n",
    "    train_test_x, train_test_y = mnist.train.next_batch(10000)\n",
    "\n",
    "    for epoch in range(15):\n",
    "        total_cost = 0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.8})\n",
    "            total_cost += cost_val\n",
    "            \n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'Avg. cost =', '{:.3f}'.format(total_cost/total_batch))\n",
    "        \n",
    "    print('optimization complete')\n",
    "        \n",
    "    is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    print('test accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "        #print('train accuracy:', sess.run(accuracy, feed_dict={X: train_test_x, Y: train_test_y}))\n",
    "            \n",
    "    lables = sess.run(model, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1})\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    for i in range(10):\n",
    "        subplot = fig.add_subplot(2, 5, i+1)\n",
    "        \n",
    "        subplot.set_xticks([])\n",
    "        subplot.set_yticks([])\n",
    "        \n",
    "        subplot.set_title('%d' % np.argmax(labels[i]))\n",
    "        \n",
    "        subplot.imshow(mnist.test.images[i].reshape((28, 28)), cmap.plt.cm.gray_r)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to prevent overfitting\n",
    "1. Dropout\n",
    "    - Keep the balace between weight by preventing the concentration of certian feature into certian neuron.  In order to do that we disable certian neuron in each epock randomly\n",
    "    \n",
    "    `tf.nn.dropout(x,  keep_prob,  noise_shape=None,  seed=None,  name=None)`\n",
    "      #### Args:\n",
    "        * x: A floating point tensor.\n",
    "        * keep_prob: A scalar Tensor with the same type as x. The probability that each element is kept.\n",
    "        * noise_shape: A 1-D Tensor of type int32, representing the shape for randomly generated keep/drop flags.\n",
    "        * seed: A Python integer. Used to create random seeds. See tf.set_random_seed for behavior.\n",
    "        * name: A name for this operation (optional).\n",
    "        \n",
    "2. Batch Normalization\n",
    "    - it not only prevents overfitting but also increases performance.\n",
    "    `tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None)`\n",
    "    \n",
    "     #### Args:\n",
    "        * x: Input Tensor of arbitrary dimensionality.\n",
    "        * mean: A mean Tensor.\n",
    "        * variance: A variance Tensor.\n",
    "        * offset: An offset Tensor, often denoted  in equations, or None. If present, will be added to the normalized tensor.\n",
    "        * scale: A scale Tensor, often denoted  in equations, or None. If present, the scale is applied to the normalized tensor.\n",
    "        * variance_epsilon: A small float number to avoid dividing by 0.\n",
    "        * name: A name for this operation (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
